{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCIlVXyCQRlB"
      },
      "source": [
        "# Indas X-Ray Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpoGmfmFQZD7"
      },
      "source": [
        "## Setup the Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm-EsyxdppOE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "import cv2 as cv2\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from skimage import measure\n",
        "from skimage import data, filters, color, morphology\n",
        "from skimage.morphology import disk\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = [10, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1VXjCSvoiVd"
      },
      "outputs": [],
      "source": [
        "matplotlib.rcParams['figure.figsize'] = [10, 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "define path values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "onlyfiles = [f for f in listdir('/content/drive/MyDrive/indas/004_OK/') if isfile(join('/content/drive/MyDrive/indas/004_OK/', f))]\n",
        "onlyfiles_test = [f for f in listdir('/content/drive/MyDrive/indas/004_NOK/') if isfile(join('/content/drive/MyDrive/indas/004_NOK/', f))]\n",
        "\n",
        "my_path = '/content/drive/MyDrive/indas/004_OK/'\n",
        "my_path_test = '/content/drive/MyDrive/indas/004_NOK/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujdjoqgyNmw3"
      },
      "source": [
        "In the following i loaded data from one day so, that there is as least difference between train and test as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7yM9ulSJg7a"
      },
      "outputs": [],
      "source": [
        "# in my direction is one other file... -> delete from list\n",
        "del onlyfiles[999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwMIote8FD2X"
      },
      "outputs": [],
      "source": [
        "Title_Info_Good = pd.DataFrame([tmp_str.split('_') for tmp_str in onlyfiles]).rename(columns = {0: 'Code_A',  \n",
        "                                                                                   1: 'Date',\n",
        "                                                                                   2: 'Code_B',\n",
        "                                                                                   3: 'Angle',\n",
        "                                                                                   4: 'Set',\n",
        "                                                                                   5: 'Res'})\n",
        "Title_Info_Good['Date'] = pd.to_datetime(Title_Info_Good.Date)\n",
        "Title_Info_Good['Res'] = Title_Info_Good.Res.str.split('.', expand = True)[0]\n",
        "Title_Info_Good['Set'] = Title_Info_Good.Set.str.split('.', expand = True)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnXbta6ZGKXk"
      },
      "outputs": [],
      "source": [
        "Title_Info_Anomal = pd.DataFrame([tmp_str.split('_') for tmp_str in onlyfiles_test]).rename(columns = {0: 'Code_A',  \n",
        "                                                                                   1: 'Date',\n",
        "                                                                                   2: 'Code_B',\n",
        "                                                                                   3: 'Angle',\n",
        "                                                                                   4: 'Set',\n",
        "                                                                                   5: 'Res'})\n",
        "Title_Info_Anomal['Date'] = pd.to_datetime(Title_Info_Anomal.Date)\n",
        "Title_Info_Anomal['Res'] = Title_Info_Anomal.Res.str.split('.', expand = True)[0]\n",
        "Title_Info_Anomal['Set'] = Title_Info_Anomal.Set.str.split('.', expand = True)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU8FGLzBJ2HL"
      },
      "outputs": [],
      "source": [
        "ind_bool_train = (Title_Info_Good.Date == dt.datetime(2022, 1, 28)).values\n",
        "files_rel_train = [onlyfiles[i] for i in range(len(onlyfiles)) if ind_bool_train[i] ]\n",
        "image_list_train = [cv2.imread( my_path + img_name, cv2.IMREAD_GRAYSCALE) for  img_name in files_rel_train[0:50]]\n",
        "# image_list_test = [cv2.imread( my_path + img_name, cv2.IMREAD_REDUCED_GRAYSCALE_4).astype('float32') / 255. for  img_name in  onlyfiles[400: 500]]\n",
        "ind_bool_test = (Title_Info_Anomal.Date == dt.datetime(2022, 1, 28))\n",
        "files_rel_test = [onlyfiles_test[i] for i in range(len(onlyfiles_test)) if ind_bool_test[i] ]\n",
        "image_list_test = [cv2.imread( my_path_test + img_name, cv2.IMREAD_GRAYSCALE) for  img_name in files_rel_test]\n",
        "# image_list_test = [cv2.imread( my_path + img_name, cv2.IMREAD_REDUCED_GRAYSCALE_4).astype('float32') / 255. for  img_name in  onlyfiles[400: 500]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g6Jm6sPMDV_"
      },
      "source": [
        "In this loaded image_list_test, there are like 3 anomalies which are relative nice to find. These are in the images with the indeces 3, 5, 6 and are all at the right edge in the upper half. Around y = 1000. The image names say there should be errors in the images 1 and 8.... and the rest is labled ok..\n",
        "Of these test images there is also to recognize, that the images 3 and 4 are a lot harder to reconstruct because there angle differ from the train files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3ZKlmBwM3pL",
        "outputId": "74b04a0b-3d08-4145-ef27-84174b0b15f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['3137.232.529_2022.01.28_3828_004_new set.323_OK.jpg',\n",
              " '3137.232.529_2022.01.28_3897_004_new set.23_FAIL.jpg',\n",
              " '3137.232.529_2022.01.28_4029_004_new set.327_OK.jpg',\n",
              " '3137.232.529_2022.01.28_4103_004_new set.155_OK.jpg',\n",
              " '3137.232.529_2022.01.28_4129_004_new set.103_OK.jpg',\n",
              " '3137.232.529_2022.01.28_3886_004_new set.555_OK.jpg',\n",
              " '3137.232.529_2022.01.28_4089_004_new set.99_OK.jpg',\n",
              " '3137.232.529_2022.01.28_4004_004_new set.227_FAIL.jpg',\n",
              " '3137.232.529_2022.01.28_3788_004_new set.163_OK.jpg']"
            ]
          },
          "execution_count": 278,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files_rel_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEhDRD8VQczh"
      },
      "source": [
        "## General Preprocessing\n",
        "\n",
        "In this section preprocessing methods are listed and / or defined which are applied on all data, the train and the test data. This mostly refer to improve the image quality. \n",
        "\n",
        "The following procedures i looked at:\n",
        "- Histogramm equlization\n",
        "- Truncate the values and then rescale\n",
        "\n",
        "At the first glance i didnt saw a big improvement but maybe just doit again when all stand. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjNwRwH-QiD4"
      },
      "source": [
        "## Preprocess train images\n",
        "\n",
        "Here methods are mentioned to improve the train procedure. This mostly refer to the decision of which images are used for train. All images for one angle can be used theoretically, but that will cause memory issues and also not improve the method so much. The important point is to use the most diverse images for train. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivC9GI0TLh4g"
      },
      "outputs": [],
      "source": [
        "def getDiverseImages(image_name_list, path, max_mean_diff):\n",
        "  img_list_great_diff = []\n",
        "  img_names_great_diff = []\n",
        "  for img_name in image_name_list:\n",
        "    tmp_image = cv2.imread( path + img_name, cv2.IMREAD_REDUCED_GRAYSCALE_8)\n",
        "    if len(img_list_great_diff) == 0:\n",
        "      img_list_great_diff.append(tmp_image)\n",
        "      img_names_great_diff.append(img_name)\n",
        "\n",
        "    else:\n",
        "      new_img = True\n",
        "      for i in range(len(img_list_great_diff)):\n",
        "        MSE = ((img_list_great_diff[i].astype('float32') - tmp_image.astype('float32'))**2).mean()\n",
        "        if MSE < max_mean_diff:\n",
        "          new_img = False\n",
        "          break\n",
        "      if new_img:\n",
        "        img_list_great_diff.append(tmp_image)    \n",
        "        img_names_great_diff.append(img_name)\n",
        "  return(img_names_great_diff)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHJ1v7OKQmjl"
      },
      "source": [
        "## Preprocess test data\n",
        "\n",
        "Here functions are defined which are used only for the test procedure. Goal is to transform the test data so, that the reconstruction algorithm can get the best out of it. \n",
        "\n",
        "- Rotation \n",
        "- Shearing\n",
        "- Similarity measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4TLd18uMLAP"
      },
      "outputs": [],
      "source": [
        "# findBestOrientation(image, img_List_Comp,  x_span, y_span, x_shear_span, y_shear_span, step_per_dir)\n",
        "# image: image matrix 252x252\n",
        "# img_List_Comp: list of images 252x252\n",
        "# x_span, y_span: list of 2 integers, the min and max shift one direction negative the other positive\n",
        "# x_shear_span, y_shear_span: list of 2 floats, the min and max shear in x and y direction\n",
        "\n",
        "\n",
        "def findBestOrientation(image, img_List_Comp,  x_span, y_span, x_shear_span, y_shear_span, step_per_dir):\n",
        "\n",
        "  # Define the parametergrid\n",
        "\n",
        "  x_shear = [x_shear_span[0] + i*(x_shear_span[1] - x_shear_span[0])/ (step_per_dir-1) for i in range(step_per_dir)]\n",
        "  y_shear = [y_shear_span[0] + i*(y_shear_span[1] - y_shear_span[0])/ (step_per_dir-1) for i in range(step_per_dir)]\n",
        "  x_steps = [(x_span[0] + np.round(i*(x_span[1] - x_span[0])/ (step_per_dir-1))).astype('int') for i in range(step_per_dir)]\n",
        "  y_steps = [(y_span[0] + np.round(i*(y_span[1] - y_span[0])/ (step_per_dir-1))).astype('int') for i in range(step_per_dir)]\n",
        "  para =[]\n",
        "  res = []\n",
        "  near = []\n",
        "\n",
        "  # for loops for all parameter combinations\n",
        "\n",
        "  for xsh in x_shear:\n",
        "    for ysh in y_shear:\n",
        "\n",
        "      # if shear x and y are defined\n",
        "      # calculate the sheared image\n",
        "\n",
        "      M = np.float32([[1, xsh, 0],\n",
        "             \t[ysh, 1  , 0],\n",
        "            \t[0, 0  , 1]])          \n",
        "      sheared_img = cv2.warpPerspective(image,M,image.shape)\n",
        "      for xst in x_steps:\n",
        "        for yst in y_steps:\n",
        "\n",
        "          # calculate MSE for all images in the list \n",
        "\n",
        "          tmp = [ ((sheared_img[(63 + yst):(189 + yst),  (63 + xst):(189 + xst)\n",
        "                               ].astype('float32')  - img_comp[63:189, 63: 189].astype(\n",
        "                                   'float32') )**2).mean() for img_comp in  img_List_Comp]\n",
        "\n",
        "          near.append(np.argmin(np.array(tmp)))\n",
        "          res.append(min(tmp))\n",
        "          para.append([xst, yst, xsh, ysh])\n",
        "    ind = np.argmin(np.array(res))\n",
        "  return([para[ind], res[ind], near[ind]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FWCivBJLvJG"
      },
      "outputs": [],
      "source": [
        "# orientateImage(image, x_shift, y_shift, x_shear, y_shear):\n",
        "\n",
        "def orientateImage(image, x_shift, y_shift, x_shear, y_shear):\n",
        "  M = np.float32([[1, x_shear, 0],\n",
        "             \t[y_shear, 1  , 0],\n",
        "            \t[0, 0  , 1]])          \n",
        "  sheared_img = cv2.warpPerspective(image,M,image.shape)\n",
        "  res = np.zeros((image.shape[0] + 2*np.abs(y_shift), image.shape[1] + 2*np.abs(x_shift)))\n",
        "\n",
        "  res[(np.abs(y_shift) - y_shift) :  (image.shape[0] + np.abs(y_shift) - y_shift), \n",
        "      (np.abs(x_shift) - x_shift) :  (image.shape[1] + np.abs(x_shift) - x_shift)\n",
        "      ] = sheared_img\n",
        "\n",
        "  return(res[\n",
        "             np.abs(y_shift):(image.shape[0] + np.abs(y_shift)),\n",
        "              np.abs(x_shift):(image.shape[1] + np.abs(x_shift)),\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NoCS0QMQmYl"
      },
      "source": [
        "## Image Reconstruction algorithms\n",
        "\n",
        "There are several ways of reconstruct the images based on principal component analysis. The following 4 ideas will be implemented:\n",
        "\n",
        "- Complete: One image is one observation, each pixel one feature\n",
        "- Rowwise: Each Row is one observation each pixel in that row a feature\n",
        "- Colwise: Each Column is on observation and each pixel in that column is a feature\n",
        "- Subimage: The images are divided in smaler frames and each frame is one observation, each pixel in that frame is one feature. \n",
        "\n",
        "The reconstruction power of the pca is the crucial point of the whole method: If the reconstruction power is too small the image reconstruction is very bad and the reconstruction error is to noisy to get any usefull information out of it. If the reconstruction power is to high, the pca will also reconstruct the anomlies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANjZ7FgmVRhc"
      },
      "source": [
        "### Complete Image Reconstruction\n",
        "\n",
        "This method treat each image as one observation and each of the pixel as one feature. Each of the estimated principal components is therefore of size 2016*2016 and every pixel in the image is seen to be dependend of all other pixels. This is very memory intensiv and slow!\n",
        "Implementation is on the other hand streight foreward.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmSbFxGQVVv1"
      },
      "outputs": [],
      "source": [
        "class comRecon:\n",
        "  def __init__(self, n_components):\n",
        "    self.pca = PCA(n_components)\n",
        "\n",
        "  def fit(self, trainArray):\n",
        "    n, col, row = trainArray.shape\n",
        "    self.pca.fit(trainArray.reshape((n, row * col)))\n",
        "\n",
        "  def reconstruct(self, image):\n",
        "    row, col = image.shape\n",
        "    recon = self.pca.inverse_transform(\n",
        "        self.pca.transform(image.reshape((1, row * col)))\n",
        "    ).reshape((row, col))\n",
        "    recon = np.round(np.minimum(np.maximum(recon, 0), 255))\n",
        "    return( recon )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-15HfXlVWP_"
      },
      "source": [
        "## Rowwise Image Reconstruction\n",
        "\n",
        "In this method the rows are seen as different observations and the columns as there features. By the straight forward implementation which would be to train one PCA for the whole images, we face quite quickly the problem, that we need many components to reconstruct the image precisly enough. But by using so many components (~100-200) the PCA will also be able to reconstruct the anomalys. So for that the following class will reconstruct the images based of divide the rows of  the image in subgroups for which reconstruction only a few components are enough. \n",
        "\n",
        "For get a better generalization in the fitting procedure also the rows of the subgroups in the neighbourhood are used. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq2VJoDgVpue"
      },
      "outputs": [],
      "source": [
        "# Class for reconstruct images rowwise\n",
        "\n",
        "# contains the following methods:\n",
        "\n",
        "# __init__(n_subgroups, n_somponents)\n",
        "# n_subgroups: integer; defines in how many subgroups the rows are devided. \n",
        "#               For each subgroup a PCA is computed. \n",
        "#               should be an integer divisor of the row numbers of the image\n",
        "# n_components: integer; defines in how many PCA components are used for reconstruction\n",
        "# return: Nothing\n",
        "\n",
        "# fit(trainArray) \n",
        "# trainArray: 3-D numpy-array [n x row x cols]\n",
        "# return: Nothing\n",
        "\n",
        "# reconstruct(image)\n",
        "# image: [row x col]\n",
        "# return: np.array [row x col] \n",
        "\n",
        "class rowRecon:\n",
        "  def __init__(self, n_subgroups, n_components):\n",
        "      self.n_subgroups = n_subgroups\n",
        "      self.n_components =n_components\n",
        "      self.pca_row = []\n",
        "\n",
        "  # nsamples x rows x columns\n",
        "  \n",
        "  def fit(self, trainArray):\n",
        "    self.nx = trainArray.shape[1]\n",
        "\n",
        "    # For each subgroup a pca is computed\n",
        "    # The fit is done with the rows of the subgroup and the rows of the subgroups\n",
        "    # in the neighbourhood. This should make the reconstruction better in generalization\n",
        "\n",
        "    for i in range(self.n_subgroups):\n",
        "      self.pca_row.append(\n",
        "          PCA(self.n_components).fit(np.concatenate(\n",
        "            trainArray[:, max(i-2, 0)*( self.nx //self.n_subgroups) : min(i+3,self.n_subgroups +1 )*( self.nx //self.n_subgroups), :   ],\n",
        "             axis = 0 )\n",
        "          )\n",
        "          )\n",
        "      \n",
        "  def reconstruct(self, image):\n",
        "\n",
        "    # The recinstuction of the image is done by \n",
        "    # 1. divide the image in subgroups\n",
        "    # 2. transform the rows with the pca of the subgroup\n",
        "    # 3. inverse transform the result back in the space of image\n",
        "    # 4. bound the reconstruction by [0, 255] and return uint8 matrix\n",
        "\n",
        "    recon = np.zeros(image.shape)\n",
        "\n",
        "    for i in range(self.n_subgroups):\n",
        "      recon_row_tmp = self.pca_row[i].inverse_transform(\n",
        "          self.pca_row [i].transform(\n",
        "              image[\n",
        "                    (i)*( self.nx // self.n_subgroups) : (i+1)*( self.nx // self.n_subgroups),\n",
        "                    :  ]))\n",
        "      recon[(i)*( self.nx // self.n_subgroups) : (i+1)*( self.nx // self.n_subgroups), : ] = recon_row_tmp\n",
        "    \n",
        "    recon = np.round(np.minimum(np.maximum(recon, 0), 255))\n",
        "    return(recon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKHdPJBNVZ-x"
      },
      "source": [
        "## Colwise Image Reconstruction\n",
        "\n",
        "Same like above for the transposed images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cwjsb0AGc2VX"
      },
      "outputs": [],
      "source": [
        "# Just a wrapper for the function above\n",
        "\n",
        "class colRecon:\n",
        "  def __init__(self, n_subgroups, n_components):\n",
        "      self.n_subgroups = n_subgroups\n",
        "      self.n_components =n_components\n",
        "      self.col_pca = rowRecon(self.n_subgroups, self.n_components)\n",
        "\n",
        "  def fit(self, trainArray):\n",
        "    self.col_pca.fit(np.swapaxes(trainArray, 1, 2))\n",
        "  \n",
        "  def reconstruct(self, image):\n",
        "    return(\n",
        "        np.swapaxes(self.col_pca.reconstruct(np.swapaxes(image, 0, 1)), 0,1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQdp1vSdVcw6"
      },
      "source": [
        "## Subimage Reconstruction\n",
        "\n",
        "This method will use pca forsubframes of the images and afterwards it will put them back together. So the image is divided in n times n subframes and for each subframe a pca is fitted. \n",
        "\n",
        "In the fitting the pca of each subframe will also get some shifted version of the image - for better generalization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFkXd2sMavTd"
      },
      "outputs": [],
      "source": [
        "from numpy.core.multiarray import concatenate\n",
        "class subRecon:\n",
        "  def __init__(self, n_subgroups, n_components):\n",
        "      self.n_subgroups_x = n_subgroups\n",
        "      self.n_subgroups_y = n_subgroups\n",
        "      self.n_components =n_components\n",
        "      self.pca_subimage = []\n",
        "\n",
        "  def fit(self, trainArray):\n",
        "\n",
        "    self.nx = trainArray.shape[1]\n",
        "    self.ny = trainArray.shape[2]\n",
        "    n_obs = trainArray.shape[0]\n",
        "    g_x = self.nx //self.n_subgroups_x\n",
        "    g_y = self.ny //self.n_subgroups_y\n",
        "\n",
        "    # For all subgroups x\n",
        "      # For all subgroups y\n",
        "       # The original frame and overlapping neighbours are used to calculate \n",
        "       # the principal decomposition\n",
        "\n",
        "    for i in range(self.n_subgroups_x):\n",
        "      for j in range(self.n_subgroups_y):\n",
        "\n",
        "        array_list = []\n",
        "\n",
        "        array_list.append( trainArray[:, \n",
        "                          j*g_y : (j+1)*g_y, \n",
        "                          i*g_x : (i+1)*g_x   \n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        \n",
        "        # now follow all overlapping neighbourhoods\n",
        "        \n",
        "        if (i > 0):\n",
        "          array_list.append( trainArray[:, \n",
        "                          (j*g_y) : ((j+1)*g_y ), \n",
        "                          (i*g_x - g_x//2) : ((i+1)*g_x - g_x//2)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        if (j > 0) & (i > 0):\n",
        "            array_list.append( trainArray[:, \n",
        "                          (j*g_y - g_y//2) : ((j+1)*g_y -  g_y//2), \n",
        "                          (i*g_x - g_x//2) : ((i+1)*g_x - g_x//2)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        if j != 0:\n",
        "            array_list.append( trainArray[:, \n",
        "                          (j*g_y - g_y//2) : ((j+1)*g_y -  g_y//2), \n",
        "                          (i*g_x ) : ((i+1)*g_x)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        if i != (self.n_subgroups_x - 1):\n",
        "          array_list.append( trainArray[:, \n",
        "                          (j*g_y) : ((j+1)*g_y ), \n",
        "                          (i*g_x + g_x//2) : ((i+1)*g_x + g_x//2)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        if (j != (self.n_subgroups_y - 1)) & (i != (self.n_subgroups_x - 1)):\n",
        "            array_list.append( trainArray[:, \n",
        "                          (j*g_y + g_y//2) : ((j+1)*g_y +  g_y//2), \n",
        "                          (i*g_x + g_x//2) : ((i+1)*g_x + g_x//2)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        if j != (self.n_subgroups_y - 1):\n",
        "            array_list.append( trainArray[:, \n",
        "                          (j*g_y + g_y//2) : ((j+1)*g_y +  g_y//2), \n",
        "                          (i*g_x ) : ((i+1)*g_x)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        if (j != (self.n_subgroups_y - 1)) & (i > 0):\n",
        "            array_list.append( trainArray[:, \n",
        "                          (j*g_y + g_y//2) : ((j+1)*g_y +  g_y//2), \n",
        "                          (i*g_x - g_x//2) : ((i+1)*g_x - g_x//2)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "        if (j > 0) & (i != (self.n_subgroups_x - 1)):\n",
        "            array_list.append( trainArray[:, \n",
        "                          (j*g_y - g_y//2) : ((j+1)*g_y -  g_y//2), \n",
        "                          (i*g_x + g_x//2) : ((i+1)*g_x + g_x//2)\n",
        "                          ].reshape((n_obs, g_y * g_x)\n",
        "                          ).copy())\n",
        "\n",
        "        \n",
        "        self.pca_subimage.append(\n",
        "            PCA(self.n_components).fit(\n",
        "                np.concatenate(array_list, axis = 0)\n",
        "            )\n",
        "            )\n",
        "  \n",
        "  def reconstruct(self, image):\n",
        "    recon = np.zeros(image.shape)\n",
        "\n",
        "    for i in range(self.n_subgroups_x):\n",
        "      for j in range(self.n_subgroups_y):\n",
        "        recon[j*( self.ny //self.n_subgroups_y) : (j+1)*( self.ny//self.n_subgroups_y), \n",
        "              i*( self.nx //self.n_subgroups_x) : (i+1)*( self.nx//self.n_subgroups_x)] =  self.pca_subimage[i * self.n_subgroups_y + j].inverse_transform(\n",
        "                  self.pca_subimage[i * self.n_subgroups_y + j].transform(\n",
        "                      image[j*( self.ny //self.n_subgroups_y) : (j+1)*( self.ny//self.n_subgroups_y), \n",
        "                            i*( self.nx //self.n_subgroups_x) : (i+1)*( self.nx//self.n_subgroups_x)   ].reshape(\n",
        "                            ( self.ny//self.n_subgroups_y * self.nx//self.n_subgroups_x)).reshape(1, -1)\n",
        "            )\n",
        "            ).reshape((self.ny//self.n_subgroups_y, self.nx//self.n_subgroups_x))\n",
        "    recon = np.round(np.minimum(np.maximum(recon, 0), 255))\n",
        "    return(recon)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG9CH92feV2Y"
      },
      "source": [
        "## Wrapper Class with filter methods\n",
        "\n",
        "This class has the only purpose to merge the reconstruction methods with some appropriate filter methods:\n",
        "\n",
        "- all algorithms will calculate in train procedure also an Standard error Map\n",
        "- getErrors method returns the reconstruction errors over a defined threshold\n",
        "- getStadardErrors returns the standardized errors over a defined threshold\n",
        "- getFilteredErrors return a binary error map based on some aditional filterin of the standard errors\n",
        "  - the standard error map gets opend 3 times. That leads to an ommitting of errors which arent at least 7 pixels in each direction. Only the errors which survive this procedure are considered in the following filter step.\n",
        "  - the biggest distance of 2 points in each error cluster is calculated. The ratio of the number of errorpoints and the square of the istance is calculated. Based on the assuption, that solderballs tend to be more compact square or circle shaped - the number of points in one error cluster should scale quadratic with the longest distance of to points. If the ratio of error points and the squared distence is below 1/3 the cluster is not considered to be an error. \n",
        "  - the standard error map gets opend 3 times. The errors which are caused by a shift in the structure will grow together an form a big error. For each left possible error cluster the growth of the error cluster is calculated and if thats above a threshold the cluster is not considered  to be an error any more. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQBk41y7eY8u"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist\n",
        "class pcaRecon:\n",
        "  def __init__(self, type, n_subgroups, n_components):\n",
        "\n",
        "    if type == 'com':\n",
        "      self.reconstructer = comRecon(n_components)\n",
        "\n",
        "    elif type == 'row':\n",
        "      self.reconstructer  = rowRecon(n_subgroups, n_components)\n",
        "\n",
        "    elif type == 'col':\n",
        "      self.reconstructer  = colRecon(n_subgroups, n_components)\n",
        "\n",
        "    elif type == 'sub':\n",
        "      self.reconstructer  = subRecon(n_subgroups, n_components)\n",
        "  \n",
        "  def fit(self, trainArray): \n",
        "    self.reconstructer.fit(trainArray)\n",
        "    self.SEMap = np.zeros((trainArray.shape[1], trainArray.shape[2]))\n",
        "    for i in range(trainArray.shape[0]):\n",
        "      self.SEMap = self.SEMap  + np.abs(trainArray[i, :, :] - self.reconstruct(trainArray[i,:,:])) /trainArray.shape[0] \n",
        " \n",
        "  def reconstruct(self, image): \n",
        "    return(self.reconstructer.reconstruct(image))\n",
        "\n",
        "  def getErrors(self, image, raw_threshold):\n",
        "    errors = self.reconstruct(image) - image\n",
        "    errors[errors < raw_threshold] = 0\n",
        "    return(errors)\n",
        "\n",
        "  def getStandardErrors(self, image, raw_threshold, SE_threshold):\n",
        "    errors = 5 * self.getErrors(image, raw_threshold) /np.maximum(self.SEMap, 5) \n",
        "    errors[errors < SE_threshold] = 0\n",
        "    return(errors)\n",
        "\n",
        "  def getFilteredErrors(self, image, raw_threshold, SE_threshold, max_errGrow):\n",
        "    errorMap = self.getStandardErrors(image, raw_threshold, SE_threshold)\n",
        "    errorMap[errorMap > 0 ] = 1\n",
        "\n",
        "    label_map = measure.label(errorMap, background=0)\n",
        "    filteredErrorMap = np.zeros((2016, 2016))\n",
        "\n",
        "    footprint = disk(3)\n",
        "    closed_error_map = morphology.binary_closing(image = errorMap, selem = footprint)\n",
        "    label_map_closed = measure.label(closed_error_map, background=0)\n",
        "\n",
        "    opened_error_map = morphology.binary_opening(image = errorMap, selem = footprint)\n",
        "    label_map_opened = measure.label(opened_error_map, background=0)\n",
        "\n",
        "    for i in range(label_map_opened.max()):\n",
        "\n",
        "      # For all Groups of errors in the opened map \n",
        "      # Calculate the size \n",
        "      # Then calculculate the size of the clusters in the closed image\n",
        "      # which the opend error belong to\n",
        "\n",
        "      cnt_opend = np.sum(label_map_opened == (i + 1))\n",
        "      closed_labels = np.unique(label_map_closed[label_map_opened == (i + 1)])\n",
        "      cnt_closed = 0\n",
        "\n",
        "      data_matrix = np.argwhere(label_map_opened == (i + 1))\n",
        "      x_max = data_matrix[:, 0].argmax()\n",
        "      x_min = data_matrix[:, 0].argmin()\n",
        "      y_max = data_matrix[:, 1].argmax()\n",
        "      y_min = data_matrix[:, 1].argmin()\n",
        "\n",
        "      max_dist = np.max(pdist(data_matrix[ [x_max, x_min, y_max, y_min] , :]))\n",
        "\n",
        "      for j in closed_labels:\n",
        "        if j > 0:\n",
        "          cnt_closed = cnt_closed + np.sum(label_map_closed == j)\n",
        "\n",
        "\n",
        "      if ( \n",
        "          ((cnt_closed / cnt_opend) < max_errGrow) &  \n",
        "          # error dont grow\n",
        "          ((max_dist**2 )/4 < cnt_opend )\n",
        "          # errors are compact \n",
        "          ):\n",
        "          filteredErrorMap[label_map_opened == (i+1)] = 1\n",
        "\n",
        "    return(filteredErrorMap)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HjI95KtE7CP"
      },
      "source": [
        "Good running parameter setups:\n",
        "- com: not evaluatet in last days\n",
        "- col / row: threshold_raw = 20, threshold_SE = 20, err_grow = 5\n",
        "- sub: threshold_raw = 10, threshold_SE = 10, err_grow = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWSHLkHM8OfS"
      },
      "source": [
        "## Final Anomaly Detector\n",
        "\n",
        "For the final anomaly detection multiple reconstruction methods are used. Each of the reconstruction methods leads to different errormaps. All should include the true anomalies. Because each method uses an other representation of the data, the errors which occor, False anomalies, should be different. By using several reconstruction algorithm and define an error only then as error if all methods say its an error.\n",
        "\n",
        "The following improvement steps are to go:\n",
        "- include the structure growth filter from the other document\n",
        "- include an orientation preprocess\n",
        "  - implemented but not checked\n",
        "- include an heatmap plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atxXpQqIG4Qb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class anomalyDetector:\n",
        "  def __init__ (self, type_list, n_subgroups_list, n_components_list, raw_th_list,  SE_th_list, err_grow_list, optimize_orientation = False):\n",
        "    self.reconstructer = []\n",
        "    self.raw_th_list = raw_th_list\n",
        "    self.SE_th_list = SE_th_list\n",
        "    self.err_grow_list = err_grow_list\n",
        "    self.optim_flag = optimize_orientation\n",
        "\n",
        "    if optimize_orientation:\n",
        "      self.train_list_small = []\n",
        "      self.train_list = []\n",
        "\n",
        "\n",
        "    for i in range(len(type_list)):\n",
        "      self.reconstructer.append(pcaRecon(type_list[i], n_subgroups_list[i], n_components_list[i]))\n",
        "  \n",
        "  def fit(self, trainArray):\n",
        "    if self.optim_flag:\n",
        "      for i in range(trainArray.shape[0]):\n",
        "        self.train_list_small.append(cv2.resize(trainArray[i, :, :], (252, 252), interpolation = cv2.INTER_AREA))\n",
        "        self.train_list.append(trainArray[i, :, :])\n",
        "\n",
        "    for i in range(len(self.reconstructer)):\n",
        "      self.reconstructer[i].fit(trainArray)\n",
        "\n",
        "  def getFilteredErrors(self, image):\n",
        "    if self.optim_flag:\n",
        "      paras, res, neigh = findBestOrientation(cv2.resize(image, (252, 252), interpolation = cv2.INTER_AREA), self.train_list_small,\n",
        "                          [-20, 20], [-20, 20], [-0.02, 0.02], [-0.02, 0.02], 5)\n",
        "      tmp_image = orientateImage(image, 8*paras[0], 8*paras[1], paras[2], paras[3])\n",
        "      im_ind = orientateImage(np.ones(image.shape), 8*paras[0], 8*paras[1], paras[2], paras[3])\n",
        "      tmp_neigh = self.train_list[neigh]\n",
        "      tmp_image[im_ind < 1] = tmp_neigh[im_ind < 1] \n",
        "    else: \n",
        "      tmp_image = image\n",
        "\n",
        "    votes = np.zeros(image.shape)\n",
        "    for i in range(len(self.reconstructer)):\n",
        "      votes += self.reconstructer[i].getFilteredErrors(\n",
        "          image, self.raw_th_list[i], self.SE_th_list[i], self.err_grow_list[i])\n",
        "    \n",
        "    if self.optim_flag:\n",
        "      rev_ind = orientateImage(np.ones(image.shape), -8*paras[0], -8*paras[1], -paras[2], -paras[3])\n",
        "      votes = orientateImage(votes, -8*paras[0], -8*paras[1], -paras[2], -paras[3])\n",
        "      votes[rev_ind < 1] = 0\n",
        "      \n",
        "    # here the structure grow prcedurehas to be \n",
        "\n",
        "    return(votes == len(self.reconstructer))\n",
        "\n",
        "  def detect(self, image):\n",
        "    errorMap = self.getFilteredErrors(image)\n",
        "    return([errorMap.sum() > 0, errorMap])      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heatmap Visualization for Bosh "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def heatmap(image, error):\n",
        "    (x,y)=err.shape\n",
        "    hmap = np.empty((x,y,3),np.float32)\n",
        "\n",
        "    hmap[np.where((err==[True]))] = [255,0,0]\n",
        "  \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(image,cmap = 'gray')\n",
        "    ax.imshow(hmap, alpha=.5)\n",
        "    return(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "heatmap(image_list_test[3],aD.detect(image_list_test[3])[1])\n",
        "plt.show"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Indas_Full_Procedure.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "1eff7cdce08881d2acad46776c2f62ab8efe6b8a296c63371986b46b6ee35131"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
